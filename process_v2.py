#!/usr/bin/env python3
"""
Process V2 - Upload avec architecture optimisÃ©e
- Chunks plus petits (forte granularitÃ©)
- Document complet stockÃ© sÃ©parÃ©ment
- Nouvelle structure Supabase (2 tables)
"""

import os
import sys
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv

from src.azure_ocr import AzureOCRProcessor
from src.embeddings import EmbeddingGenerator
from src.supabase_client_v2 import SupabaseUploaderV2
from src.pdf_extractor import extract_text_from_pdf
from src.chunking_config import chunking_manager, get_chunking_params

# Charger les variables d'environnement
load_dotenv()

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# CONFIGURATION DE LA GRANULARITÃ‰
# ============================================================================

# Configuration automatique via chunking_config
# Par dÃ©faut : FINE (chunk_size=400, overlap=100)
# Peut Ãªtre modifiÃ© via GRANULARITY_LEVEL dans .env
# Options : ULTRA_FINE, FINE, MEDIUM, STANDARD, COARSE

config = chunking_manager.get_config()
logger.info(f"Configuration de chunking : {config}")
logger.info(f"Niveau de granularitÃ© : {chunking_manager.get_granularity_level().value.upper()}")
logger.info(f"Chunks attendus pour 10k caractÃ¨res : ~{config.chunks_per_10k}")


def extract_text_from_file(file_path: str, ocr_processor: Optional[AzureOCRProcessor] = None) -> tuple:
    """
    Extrait le texte d'un fichier (PDF, TXT, etc.).

    Returns:
        (texte_complet, mÃ©thode_utilisÃ©e, page_count)
    """
    file_name = Path(file_path).name
    file_ext = Path(file_path).suffix.lower()

    # 1. Fichiers texte
    if file_ext in ['.txt', '.md', '.csv']:
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read()

            # Nettoyer les caractÃ¨res null
            text = text.replace('\u0000', '').replace('\x00', '')

            if text.strip():
                return text, 'text_file', 0
        except Exception as e:
            raise Exception(f"Erreur lecture fichier texte: {e}")

    # 2. Fichiers PDF
    elif file_ext == '.pdf':
        # 2a. Essayer extraction directe d'abord
        text = extract_text_from_pdf(file_path)

        if text and len(text.strip()) > 100:
            # Compter les pages si possible
            try:
                from PyPDF2 import PdfReader
                reader = PdfReader(file_path)
                page_count = len(reader.pages)
            except:
                page_count = 0

            return text, 'pdf_direct', page_count

        # 2b. Fallback vers OCR si PDF scannÃ©
        if ocr_processor is None:
            raise Exception("PDF scannÃ© mais Azure OCR non disponible")

        size_mb = os.path.getsize(file_path) / (1024 * 1024)
        if size_mb > 50:
            raise Exception(f"PDF scannÃ© trop grand pour OCR: {size_mb:.1f} MB (max 50 MB)")

        try:
            result = ocr_processor.process_file(file_path)
            text = result.get('full_text', '')

            if not text or len(text.strip()) == 0:
                raise Exception("OCR n'a extrait aucun texte")

            # Nettoyer
            text = text.replace('\u0000', '').replace('\x00', '')

            return text, 'azure_ocr', result.get('page_count', 0)

        except Exception as e:
            raise Exception(f"Erreur OCR: {e}")

    else:
        raise Exception(f"Type de fichier non supportÃ©: {file_ext}")


def process_single_file(
    file_path: str,
    embedding_gen: EmbeddingGenerator,
    uploader: SupabaseUploaderV2,
    ocr_processor: Optional[AzureOCRProcessor] = None,
    upload: bool = True
) -> Dict:
    """
    Traite un seul fichier avec la nouvelle architecture.

    Returns:
        Dict avec les rÃ©sultats du traitement
    """
    file_name = Path(file_path).name

    try:
        print(f"\n{'='*70}")
        print(f"ğŸ“„ {file_name}")
        print(f"{'='*70}")

        # 1. Extraction du texte
        print(f"ğŸ“¥ Extraction du texte...")
        full_text, method, page_count = extract_text_from_file(file_path, ocr_processor)

        print(f"âœ… Texte extrait: {len(full_text)} caractÃ¨res ({method})")
        if page_count:
            print(f"ğŸ“„ Pages: {page_count}")

        # 2. DÃ©coupage en chunks (utilise la configuration globale)
        chunk_size, chunk_overlap = get_chunking_params()
        print(f"ğŸ”¢ DÃ©coupage en chunks (taille: {chunk_size}, overlap: {chunk_overlap})...")
        chunks = embedding_gen.chunk_text(full_text)

        print(f"âœ… {len(chunks)} chunks crÃ©Ã©s (granularitÃ© fine)")

        # 3. GÃ©nÃ©ration des embeddings
        print(f"ğŸ§  GÃ©nÃ©ration de {len(chunks)} embeddings...")
        embeddings = embedding_gen.generate_embeddings_batch(chunks, batch_size=100)

        print(f"âœ… {len(embeddings)} embeddings gÃ©nÃ©rÃ©s")

        # 4. PrÃ©parer les donnÃ©es
        chunks_with_embeddings = []
        for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            chunks_with_embeddings.append({
                "chunk_index": idx,
                "chunk_text": chunk,
                "embedding": embedding,
                "metadata": {
                    "total_chunks": len(chunks),
                    "chunk_size": len(chunk)
                }
            })

        # 5. Upload vers Supabase
        if upload:
            print(f"ğŸ“¤ Upload vers Supabase...")

            result = uploader.upload_document_with_chunks(
                file_path=file_path,
                full_content=full_text,
                chunks_with_embeddings=chunks_with_embeddings,
                file_type=Path(file_path).suffix.lstrip('.'),
                page_count=page_count,
                processing_method=method,
                additional_metadata={
                    "original_file_name": file_name,
                    "chunk_size": chunk_size,
                    "chunk_overlap": chunk_overlap,
                    "granularity_level": chunking_manager.get_granularity_level().value
                }
            )

            print(f"âœ… Upload terminÃ©: {result['chunks_count']} chunks")

        return {
            "status": "success",
            "file_name": file_name,
            "full_text_length": len(full_text),
            "chunks_count": len(chunks),
            "embeddings_count": len(embeddings),
            "method": method,
            "page_count": page_count
        }

    except Exception as e:
        error_msg = str(e)
        print(f"âŒ Erreur: {error_msg}")

        return {
            "status": "error",
            "file_name": file_name,
            "error": error_msg
        }


def process_files_parallel(
    file_paths: List[str],
    embedding_gen: EmbeddingGenerator,
    uploader: SupabaseUploaderV2,
    ocr_processor: Optional[AzureOCRProcessor] = None,
    upload: bool = True,
    workers: int = 3
) -> Dict:
    """
    Traite plusieurs fichiers en parallÃ¨le.
    """
    results = {
        "success": [],
        "errors": []
    }

    total = len(file_paths)

    chunk_size, chunk_overlap = get_chunking_params()
    granularity = chunking_manager.get_granularity_level().value.upper()

    print(f"\n{'='*70}")
    print(f"ğŸš€ TRAITEMENT DE {total} FICHIERS")
    print(f"   Workers: {workers}")
    print(f"   Niveau de granularitÃ©: {granularity}")
    print(f"   Taille chunk: {chunk_size} caractÃ¨res (overlap {chunk_overlap})")
    print(f"   Upload: {'OUI' if upload else 'NON'}")
    print(f"{'='*70}\n")

    with ThreadPoolExecutor(max_workers=workers) as executor:
        # Soumettre tous les fichiers
        future_to_file = {
            executor.submit(
                process_single_file,
                file_path,
                embedding_gen,
                uploader,
                ocr_processor,
                upload
            ): file_path
            for file_path in file_paths
        }

        # Traiter les rÃ©sultats au fur et Ã  mesure
        completed = 0
        for future in as_completed(future_to_file):
            completed += 1
            file_path = future_to_file[future]

            try:
                result = future.result()

                if result["status"] == "success":
                    results["success"].append(result)
                    print(f"\n[{completed}/{total}] âœ… {result['file_name']}: {result['chunks_count']} chunks")
                else:
                    results["errors"].append(result)
                    print(f"\n[{completed}/{total}] âŒ {result['file_name']}: {result['error']}")

            except Exception as e:
                results["errors"].append({
                    "status": "error",
                    "file_name": Path(file_path).name,
                    "error": str(e)
                })
                print(f"\n[{completed}/{total}] âŒ {Path(file_path).name}: {e}")

    return results


def main():
    """Point d'entrÃ©e principal."""
    parser = argparse.ArgumentParser(
        description="Process V2 - Upload avec forte granularitÃ© et architecture optimisÃ©e"
    )

    parser.add_argument(
        "-i", "--input",
        required=True,
        help="Dossier ou fichier Ã  traiter"
    )

    parser.add_argument(
        "--upload",
        action="store_true",
        help="Upload vers Supabase"
    )

    parser.add_argument(
        "--workers",
        type=int,
        default=3,
        help="Nombre de workers parallÃ¨les (dÃ©faut: 3)"
    )

    parser.add_argument(
        "--max-files",
        type=int,
        default=None,
        help="Nombre maximum de fichiers Ã  traiter"
    )

    parser.add_argument(
        "--extensions",
        type=str,
        default="pdf,txt,md,csv",
        help="Extensions de fichiers Ã  traiter (sÃ©parÃ©es par des virgules)"
    )

    args = parser.parse_args()

    # Initialiser les composants
    try:
        print("ğŸ”§ Initialisation...")

        # Embeddings
        embedding_gen = EmbeddingGenerator()
        print("âœ… GÃ©nÃ©rateur d'embeddings initialisÃ©")

        # Supabase V2
        uploader = SupabaseUploaderV2()
        print("âœ… Client Supabase V2 initialisÃ©")

        # Azure OCR (optionnel)
        ocr_processor = None
        try:
            ocr_processor = AzureOCRProcessor()
            print("âœ… Azure OCR initialisÃ©")
        except:
            print("âš ï¸  Azure OCR non disponible (PDFs scannÃ©s non supportÃ©s)")

    except Exception as e:
        print(f"âŒ Erreur initialisation: {e}")
        sys.exit(1)

    # Collecter les fichiers
    input_path = Path(args.input)
    file_paths = []

    extensions = args.extensions.split(',')
    extensions = [ext.strip().lower() for ext in extensions]

    if input_path.is_file():
        file_paths = [str(input_path)]
    elif input_path.is_dir():
        for ext in extensions:
            pattern = f"**/*.{ext}" if not ext.startswith('.') else f"**/*{ext}"
            file_paths.extend([str(f) for f in input_path.glob(pattern)])

        # Limiter si demandÃ©
        if args.max_files:
            file_paths = file_paths[:args.max_files]
    else:
        print(f"âŒ Chemin invalide: {input_path}")
        sys.exit(1)

    if not file_paths:
        print(f"âŒ Aucun fichier trouvÃ© dans {input_path}")
        sys.exit(1)

    print(f"ğŸ“ {len(file_paths)} fichiers trouvÃ©s\n")

    # Traiter
    results = process_files_parallel(
        file_paths=file_paths,
        embedding_gen=embedding_gen,
        uploader=uploader,
        ocr_processor=ocr_processor,
        upload=args.upload,
        workers=args.workers
    )

    # RÃ©sumÃ©
    print(f"\n{'='*70}")
    print(f"ğŸ“Š RÃ‰SUMÃ‰")
    print(f"{'='*70}")
    print(f"âœ… SuccÃ¨s: {len(results['success'])}")
    print(f"âŒ Erreurs: {len(results['errors'])}")
    print(f"ğŸ“ Total: {len(results['success']) + len(results['errors'])}")

    if results['success']:
        total_chunks = sum(r['chunks_count'] for r in results['success'])
        print(f"\nğŸ”¢ Total embeddings crÃ©Ã©s: {total_chunks}")
        print(f"ğŸ“Š Moyenne par document: {total_chunks / len(results['success']):.1f}")

    if results['errors']:
        print(f"\nâŒ Fichiers en erreur:")
        for error in results['errors']:
            print(f"   - {error['file_name']}: {error['error']}")

    if args.upload:
        # Afficher les stats
        stats = uploader.get_database_stats()
        print(f"\nğŸ’¾ Statistiques Supabase:")
        print(f"   Documents: {stats.get('total_documents', 0)}")
        print(f"   Chunks: {stats.get('total_chunks', 0)}")
        print(f"   Moyenne chunks/doc: {stats.get('avg_chunks_per_document', 0):.1f}")
        print(f"   Taille moyenne chunk: {stats.get('avg_chunk_size', 0)} caractÃ¨res")

    print(f"\nğŸ‰ TerminÃ© !")


if __name__ == "__main__":
    main()
