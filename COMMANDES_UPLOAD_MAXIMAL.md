# ðŸš€ COMMANDES POUR UPLOAD MAXIMAL

## ðŸ“ Tes Documents
**Chemin:** `C:\OneDriveExport`

---

## âš¡ COMMANDES Ã€ EXÃ‰CUTER

### 1ï¸âƒ£ Vider les Anciennes Tables (Dans Supabase SQL Editor)

**Va sur Supabase â†’ SQL Editor â†’ ExÃ©cute :**

```sql
-- Vider toutes les tables
TRUNCATE TABLE document_chunks CASCADE;
TRUNCATE TABLE documents_full CASCADE;
TRUNCATE TABLE extracted_entities CASCADE;
TRUNCATE TABLE document_tag_relations CASCADE;
TRUNCATE TABLE document_tags CASCADE;
TRUNCATE TABLE document_relations CASCADE;

-- VÃ©rification (doit retourner 0 pour chaque)
SELECT 'documents_full' as table_name, COUNT(*) as count FROM documents_full
UNION ALL
SELECT 'document_chunks', COUNT(*) FROM document_chunks
UNION ALL
SELECT 'extracted_entities', COUNT(*) FROM extracted_entities
UNION ALL
SELECT 'document_tags', COUNT(*) FROM document_tags;
```

âœ… **Tu dois voir 0 partout**

---

### 2ï¸âƒ£ Upload MAXIMAL

**Dans ton terminal (WSL/Linux) :**

```bash
cd /home/user/embeddingsall

# Test d'abord (DRY RUN) - SANS upload rÃ©el
python upload_maximal.py -i /mnt/c/OneDriveExport --dry-run
```

**OU si tu es sur Windows (cmd/PowerShell) :**

```bash
cd C:\path\to\embeddingsall

# Test d'abord
python upload_maximal.py -i "C:\OneDriveExport" --dry-run
```

---

### 3ï¸âƒ£ Si le Test est OK â†’ Upload RÃ‰EL

**WSL/Linux :**

```bash
python upload_maximal.py -i /mnt/c/OneDriveExport
```

**Windows :**

```bash
python upload_maximal.py -i "C:\OneDriveExport"
```

---

## ðŸ“Š Configuration MAXIMALE AppliquÃ©e

```
âœ… Chunk Size: 800 caractÃ¨res (plus petit = meilleure prÃ©cision)
âœ… Overlap: 250 caractÃ¨res (plus grand = meilleur contexte)
âœ… Contexte: 300 caractÃ¨res avant/aprÃ¨s (au lieu de 200)
âœ… Extraction: TOUTES les mÃ©tadonnÃ©es (100+ champs)
âœ… EntitÃ©s: Entreprises, lieux, personnes, montants, dates
âœ… Tags: GÃ©nÃ©ration automatique intelligente
âœ… Scores: Importance, qualitÃ©, complÃ©tude
âœ… Structure: Titres, sections, paragraphes
âœ… Analyse: Langue, formalitÃ©, type de document
âœ… Full-text: Vecteurs de recherche pondÃ©rÃ©s
âœ… Logging: Mode DEBUG complet (fichier upload_maximal.log)
```

---

## ðŸ“ Pendant l'Upload

Tu verras des logs dÃ©taillÃ©s comme :

```
================================================================================
UPLOAD MAXIMAL: /mnt/c/OneDriveExport/contrat_lausanne.pdf
================================================================================
ðŸ“„ Extraction du texte...
âœ… Texte extrait: 15234 caractÃ¨res, 2456 mots
ðŸ” Extraction MAXIMALE des mÃ©tadonnÃ©es...
âœ… 87 champs de mÃ©tadonnÃ©es extraits
ðŸ—‚ï¸  Mapping vers schÃ©ma de base de donnÃ©es...
   Type: contrat de location
   CatÃ©gorie: immobilier
   Commune: Lausanne
   Canton: VD
   Tags: 12 tags
   Score complÃ©tude: 89.3%
   Score richesse: 92.1%
ðŸ“¤ Upload du document vers Supabase...
âœ… Document uploadÃ©: ID=42
âœ‚ï¸  CrÃ©ation des chunks enrichis...
âœ… 19 chunks crÃ©Ã©s
   ðŸŒŸ 7 chunks avec importance > 0.7
   ðŸ’° 5 chunks contiennent des montants
ðŸ“¤ Upload des chunks vers Supabase...
âœ… 19 chunks uploadÃ©s
ðŸ¢ 23 entitÃ©s uniques extraites
ðŸ·ï¸  12 tags crÃ©Ã©s
â±ï¸  Temps de traitement: 8.34 secondes
================================================================================
âœ… UPLOAD TERMINÃ‰ AVEC SUCCÃˆS
================================================================================
```

---

## â±ï¸ Estimation du Temps

**Calcul :**
- ~5-10 secondes par document (dÃ©pend de la taille)
- Si tu as 100 documents â†’ ~10-15 minutes
- Si tu as 1000 documents â†’ ~2-3 heures
- Si tu as 10000 documents â†’ ~20-30 heures

**L'upload se fait automatiquement**, tu peux lancer et laisser tourner !

---

## ðŸ“Š Ã€ la Fin - Statistiques

```
================================================================================
ðŸ“Š STATISTIQUES DÃ‰TAILLÃ‰ES D'UPLOAD MAXIMAL
================================================================================
Documents traitÃ©s:        1234
Documents uploadÃ©s:       1230
Chunks crÃ©Ã©s:             24680
EntitÃ©s extraites:        5432
Tags crÃ©Ã©s:               14760
Champs mÃ©tadonnÃ©es:       107310
Taille totale:            456.78 MB
Temps total:              183.45 minutes

Moyennes par document:
  - Temps:                8.95 secondes
  - Chunks:               20.1
  - MÃ©tadonnÃ©es:          87.2

Erreurs:                  4
================================================================================
```

---

## ðŸ” VÃ©rifier dans Supabase

**AprÃ¨s l'upload, va dans Supabase â†’ Table Editor :**

### Table `documents_full`

Tu devrais voir pour chaque document :
- âœ… `file_name` : nom du fichier
- âœ… `type_document` : dÃ©tectÃ© automatiquement
- âœ… `categorie` : catÃ©gorie principale
- âœ… `commune`, `canton` : localisation
- âœ… `montant_principal` : montant dÃ©tectÃ©
- âœ… `date_document` : date du document
- âœ… `tags` : array de tags
- âœ… `metadata_completeness_score` : score de complÃ©tude
- âœ… Etc. (40+ champs remplis)

### Table `document_chunks`

Tu devrais voir pour chaque chunk :
- âœ… `chunk_content` : contenu du chunk
- âœ… `context_before` : 300 chars avant
- âœ… `context_after` : 300 chars aprÃ¨s
- âœ… `importance_score` : score d'importance
- âœ… `has_tables`, `has_amounts`, `has_dates` : flags
- âœ… `entities_mentioned` : entitÃ©s extraites
- âœ… `embedding` : vecteur d'embedding

---

## ðŸ§ª Tester la Recherche

**Dans un script Python ou notebook :**

```python
from src.supabase_client_enhanced import SupabaseClientEnhanced
from src.embeddings import generate_embedding

client = SupabaseClientEnhanced()

# Test 1: Recherche full-text
print("ðŸ” Test 1: Recherche full-text")
results = client.search_fulltext('contrat location lausanne', limit=5)
print(f"TrouvÃ© {len(results)} documents")
for r in results:
    print(f"  ðŸ“„ {r['file_name']}")
    print(f"     Type: {r['type_document']}, Commune: {r['commune']}")
    print(f"     Score: {r['rank']:.3f}")
    print()

# Test 2: Recherche sÃ©mantique
print("ðŸ” Test 2: Recherche sÃ©mantique")
query = "Trouver les Ã©valuations immobiliÃ¨res de plus de 10 millions"
embedding = generate_embedding(query)
results = client.search_similar(embedding, limit=5)
print(f"TrouvÃ© {len(results)} chunks")
for r in results:
    print(f"  ðŸ“„ {r['file_name']}")
    print(f"     Type: {r['type_document']}, Montant: {r.get('montant_principal', 'N/A')} CHF")
    print(f"     SimilaritÃ©: {r['similarity']:.2%}")
    print(f"     Extrait: {r['chunk_content'][:100]}...")
    print()

# Test 3: Recherche hybride (meilleure prÃ©cision)
print("ðŸ” Test 3: Recherche hybride")
results = client.search_hybrid(
    search_text='Ã©valuation immobiliÃ¨re aigle',
    query_embedding=embedding,
    limit=5
)
print(f"TrouvÃ© {len(results)} rÃ©sultats")
for r in results:
    print(f"  ðŸ“„ {r['file_name']}")
    print(f"     Score combinÃ©: {r['combined_score']:.3f}")
    print(f"     (SÃ©mantique: {r['semantic_score']:.3f}, Full-text: {r['fulltext_score']:.3f})")
    print()

# Test 4: Statistiques
print("ðŸ“Š Statistiques")
stats_cat = client.get_stats_by_category()
print(f"CatÃ©gories: {len(stats_cat)}")
for s in stats_cat[:5]:
    print(f"  {s['categorie']} / {s['type_document']}: {s['document_count']} docs")

stats_loc = client.get_stats_by_location()
print(f"\nLocalisations: {len(stats_loc)}")
for s in stats_loc[:5]:
    print(f"  {s['canton']} - {s['commune']}: {s['document_count']} docs")

# RafraÃ®chir les vues matÃ©rialisÃ©es
print("\nðŸ”„ RafraÃ®chissement des vues matÃ©rialisÃ©es...")
client.refresh_materialized_views()
print("âœ… Fait")
```

---

## ðŸ’° CoÃ»t EstimÃ© OpenAI

**Estimation :**
- ModÃ¨le: `text-embedding-3-small`
- Prix: ~$0.00002 par 1000 tokens
- 1 chunk â‰ˆ 200 tokens en moyenne
- 1000 documents Ã— 20 chunks = 20000 chunks
- 20000 chunks Ã— 200 tokens = 4M tokens
- CoÃ»t: ~$0.08 pour 1000 documents

**Pour 10000 documents â‰ˆ $0.80**

---

## ðŸ—‚ï¸ Fichier de Log

Tous les dÃ©tails sont sauvegardÃ©s dans :

```bash
upload_maximal.log
```

Tu peux le consulter pour voir exactement ce qui s'est passÃ©.

---

## ðŸ†˜ En Cas d'Erreur

### Erreur: "No module named 'src'"

```bash
# Assure-toi d'Ãªtre dans le bon rÃ©pertoire
cd /home/user/embeddingsall

# VÃ©rifie que le dossier src/ existe
ls -la src/
```

### Erreur: "OPENAI_API_KEY not found"

```bash
# VÃ©rifie ton fichier .env
cat .env | grep OPENAI_API_KEY

# Si vide, ajoute-le
echo "OPENAI_API_KEY=sk-xxx..." >> .env
```

### Erreur: "SUPABASE_URL not found"

```bash
# VÃ©rifie ton fichier .env
cat .env | grep SUPABASE

# Ajoute si nÃ©cessaire
echo "SUPABASE_URL=https://xxx.supabase.co" >> .env
echo "SUPABASE_KEY=eyJxxx..." >> .env
```

### Upload bloquÃ© / TrÃ¨s lent

C'est normal si tu as beaucoup de fichiers. L'upload prend du temps car :
1. Extraction de texte (PDF, DOCX, etc.)
2. Analyse de 100+ mÃ©tadonnÃ©es
3. GÃ©nÃ©ration des embeddings (API OpenAI)
4. Upload vers Supabase

**Laisse tourner**, Ã§a continue mÃªme si Ã§a semble lent.

---

## ðŸŽ¯ COMMANDE FINALE

**Si tu es sur WSL/Linux :**

```bash
cd /home/user/embeddingsall
python upload_maximal.py -i /mnt/c/OneDriveExport
```

**Si tu es sur Windows :**

```bash
cd C:\...\embeddingsall
python upload_maximal.py -i "C:\OneDriveExport"
```

---

**C'est parti ! Lance la commande et laisse tourner ! ðŸš€**

**N'oublie pas** :
1. âœ… Vider les tables dans Supabase d'abord
2. âœ… VÃ©rifier que .env contient OPENAI_API_KEY et SUPABASE_URL/KEY
3. âœ… Lancer l'upload
4. âœ… Attendre (peut prendre plusieurs heures si beaucoup de fichiers)
5. âœ… VÃ©rifier dans Supabase Table Editor
6. âœ… Tester la recherche
