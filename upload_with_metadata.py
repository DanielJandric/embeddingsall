#!/usr/bin/env python3
"""
Script d'upload de documents avec m√©tadonn√©es enrichies.

Ce script permet d'uploader des documents en sp√©cifiant des m√©tadonn√©es
personnalis√©es via un fichier CSV ou JSON.

Formats support√©s :
1. CSV avec colonnes : file_path, type_document, metadata (JSON)
2. JSON avec configuration compl√®te
3. Dossiers organis√©s avec m√©tadonn√©es h√©rit√©es
"""

import argparse
import json
import csv
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv

load_dotenv()

from src.metadata_enrichment import create_metadata_for_document, MetadataExtractor
from src.embeddings import EmbeddingGenerator
from src.supabase_client_v2 import SupabaseUploaderV2
from src.azure_ocr import AzureOCRProcessor
from src.chunking_config import get_chunking_params
from process_v2 import extract_text_from_file

# Configuration du logging
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_metadata_from_csv(csv_path: str) -> Dict[str, Dict[str, Any]]:
    """
    Charge les m√©tadonn√©es depuis un fichier CSV.

    Format CSV attendu :
    file_path,type_document,commune,valeur_chf,annee,tags
    C:\Docs\eval1.pdf,evaluation_immobiliere,Aigle,14850000,2023,"immobilier,vaud"

    Returns:
        Dictionnaire {file_path: metadata}
    """
    metadata_map = {}

    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)

        for row in reader:
            file_path = row.pop('file_path')

            # Convertir les valeurs num√©riques
            metadata = {}
            for key, value in row.items():
                if not value or value.strip() == '':
                    continue

                # Essayer de convertir en nombre
                try:
                    if '.' in value:
                        metadata[key] = float(value)
                    else:
                        metadata[key] = int(value)
                except ValueError:
                    # Essayer de parser JSON (pour listes, etc.)
                    if value.startswith('[') or value.startswith('{'):
                        try:
                            metadata[key] = json.loads(value)
                        except:
                            metadata[key] = value
                    else:
                        metadata[key] = value

            metadata_map[file_path] = metadata

    logger.info(f"‚úÖ {len(metadata_map)} fichiers avec m√©tadonn√©es charg√©s depuis CSV")
    return metadata_map


def load_metadata_from_json(json_path: str) -> Dict[str, Dict[str, Any]]:
    """
    Charge les m√©tadonn√©es depuis un fichier JSON.

    Format JSON attendu :
    {
        "C:\\Docs\\eval1.pdf": {
            "type_document": "evaluation_immobiliere",
            "commune": "Aigle",
            "valeur_chf": 14850000,
            ...
        },
        ...
    }
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        metadata_map = json.load(f)

    logger.info(f"‚úÖ {len(metadata_map)} fichiers avec m√©tadonn√©es charg√©s depuis JSON")
    return metadata_map


def create_metadata_csv_template(output_path: str, file_paths: List[str]):
    """
    Cr√©e un template CSV √† remplir manuellement.

    Args:
        output_path: Chemin du fichier CSV √† cr√©er
        file_paths: Liste des fichiers √† traiter
    """
    # Colonnes recommand√©es
    fieldnames = [
        'file_path',
        'type_document',
        'commune',
        'canton',
        'annee',
        'valeur_chf',
        'surface_m2',
        'description',
        'tags'
    ]

    with open(output_path, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for file_path in file_paths:
            # Extraction automatique basique
            auto_meta = MetadataExtractor.extract_from_filename(file_path)

            row = {
                'file_path': file_path,
                'type_document': auto_meta.get('type_document', ''),
                'commune': auto_meta.get('commune', ''),
                'canton': auto_meta.get('canton', ''),
                'annee': auto_meta.get('annee', ''),
                'valeur_chf': '',
                'surface_m2': '',
                'description': '',
                'tags': ''
            }
            writer.writerow(row)

    logger.info(f"‚úÖ Template CSV cr√©√© : {output_path}")
    logger.info(f"üìù Remplissez les colonnes manquantes puis relancez avec --metadata-csv")


def process_with_metadata(
    file_path: str,
    metadata: Dict[str, Any],
    embedding_gen: EmbeddingGenerator,
    uploader: SupabaseUploaderV2,
    ocr_processor: Optional[AzureOCRProcessor] = None
) -> Dict[str, Any]:
    """
    Traite un fichier avec des m√©tadonn√©es enrichies.
    """
    file_name = Path(file_path).name

    try:
        logger.info(f"\n{'='*70}")
        logger.info(f"üìÑ {file_name}")
        logger.info(f"{'='*70}")

        # 1. Extraction du texte
        logger.info(f"üì• Extraction du texte...")
        full_text, method, page_count = extract_text_from_file(file_path, ocr_processor)
        logger.info(f"‚úÖ Texte extrait: {len(full_text)} caract√®res ({method})")

        # 2. Extraction automatique de m√©tadonn√©es depuis le contenu
        content_metadata = MetadataExtractor.extract_from_content(full_text)
        logger.info(f"üìä M√©tadonn√©es extraites du contenu: {len(content_metadata)} champs")

        # 3. Fusionner toutes les m√©tadonn√©es (priorit√© : manuelles > contenu > fichier)
        filename_metadata = MetadataExtractor.extract_from_filename(file_path)
        final_metadata = {**filename_metadata, **content_metadata, **metadata}

        logger.info(f"‚úÖ M√©tadonn√©es finales: {len(final_metadata)} champs")
        logger.info(f"   Principaux champs: {list(final_metadata.keys())[:8]}")

        # 4. D√©coupage en chunks
        chunk_size, chunk_overlap = get_chunking_params()
        logger.info(f"üî¢ D√©coupage en chunks (taille: {chunk_size}, overlap: {chunk_overlap})...")
        chunks = embedding_gen.chunk_text(full_text)
        logger.info(f"‚úÖ {len(chunks)} chunks cr√©√©s")

        # 5. G√©n√©ration des embeddings
        logger.info(f"üß† G√©n√©ration de {len(chunks)} embeddings...")
        embeddings = embedding_gen.generate_embeddings_batch(chunks, batch_size=100)
        logger.info(f"‚úÖ {len(embeddings)} embeddings g√©n√©r√©s")

        # 6. Pr√©parer les donn√©es avec m√©tadonn√©es enrichies
        chunks_with_embeddings = []
        for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            chunk_metadata = final_metadata.copy()
            chunk_metadata.update({
                "total_chunks": len(chunks),
                "chunk_size": len(chunk)
            })

            chunks_with_embeddings.append({
                "chunk_index": idx,
                "chunk_text": chunk,
                "embedding": embedding,
                "metadata": chunk_metadata
            })

        # 7. Upload vers Supabase
        logger.info(f"üì§ Upload vers Supabase avec m√©tadonn√©es enrichies...")

        result = uploader.upload_document_with_chunks(
            file_path=file_path,
            full_content=full_text,
            chunks_with_embeddings=chunks_with_embeddings,
            file_type=Path(file_path).suffix.lstrip('.'),
            page_count=page_count,
            processing_method=method,
            additional_metadata=final_metadata  # M√©tadonn√©es pour le document complet
        )

        logger.info(f"‚úÖ Upload termin√©: {result['chunks_count']} chunks avec m√©tadonn√©es enrichies")

        return {
            "status": "success",
            "file_name": file_name,
            "metadata_fields": len(final_metadata),
            "chunks_count": len(chunks),
            "sample_metadata": dict(list(final_metadata.items())[:5])  # Premier 5 champs
        }

    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}")
        return {
            "status": "error",
            "file_name": file_name,
            "error": str(e)
        }


def main():
    parser = argparse.ArgumentParser(
        description="Upload de documents avec m√©tadonn√©es enrichies"
    )

    parser.add_argument(
        "-i", "--input",
        help="Dossier ou fichier √† traiter"
    )

    parser.add_argument(
        "--metadata-csv",
        help="Fichier CSV contenant les m√©tadonn√©es"
    )

    parser.add_argument(
        "--metadata-json",
        help="Fichier JSON contenant les m√©tadonn√©es"
    )

    parser.add_argument(
        "--create-template",
        action="store_true",
        help="Cr√©er un template CSV pour remplir les m√©tadonn√©es"
    )

    parser.add_argument(
        "--template-output",
        default="metadata_template.csv",
        help="Nom du fichier template CSV (d√©faut: metadata_template.csv)"
    )

    parser.add_argument(
        "--extensions",
        type=str,
        default="pdf,txt,md",
        help="Extensions de fichiers √† traiter (s√©par√©es par des virgules)"
    )

    args = parser.parse_args()

    # Initialiser les composants
    try:
        logger.info("üîß Initialisation...")
        embedding_gen = EmbeddingGenerator()
        uploader = SupabaseUploaderV2()
        logger.info("‚úÖ Composants initialis√©s")

        try:
            ocr_processor = AzureOCRProcessor()
            logger.info("‚úÖ Azure OCR initialis√©")
        except:
            ocr_processor = None
            logger.warning("‚ö†Ô∏è  Azure OCR non disponible")

    except Exception as e:
        logger.error(f"‚ùå Erreur initialisation: {e}")
        sys.exit(1)

    # Collecter les fichiers
    if not args.input:
        logger.error("‚ùå Argument --input requis")
        sys.exit(1)

    input_path = Path(args.input)
    file_paths = []
    extensions = args.extensions.split(',')
    extensions = [ext.strip().lower() for ext in extensions]

    if input_path.is_file():
        file_paths = [str(input_path)]
    elif input_path.is_dir():
        for ext in extensions:
            pattern = f"**/*.{ext}" if not ext.startswith('.') else f"**/*{ext}"
            file_paths.extend([str(f) for f in input_path.glob(pattern)])
    else:
        logger.error(f"‚ùå Chemin invalide: {input_path}")
        sys.exit(1)

    logger.info(f"üìÅ {len(file_paths)} fichiers trouv√©s")

    # Mode 1 : Cr√©er un template CSV
    if args.create_template:
        create_metadata_csv_template(args.template_output, file_paths)
        logger.info(f"\nüìù Prochaines √©tapes:")
        logger.info(f"1. Ouvrir {args.template_output} dans Excel")
        logger.info(f"2. Remplir les colonnes avec les bonnes m√©tadonn√©es")
        logger.info(f"3. Sauvegarder le fichier")
        logger.info(f"4. Relancer : python upload_with_metadata.py -i ... --metadata-csv {args.template_output}")
        return

    # Mode 2 : Upload avec m√©tadonn√©es
    metadata_map = {}

    if args.metadata_csv:
        metadata_map = load_metadata_from_csv(args.metadata_csv)
    elif args.metadata_json:
        metadata_map = load_metadata_from_json(args.metadata_json)
    else:
        logger.warning("‚ö†Ô∏è  Aucun fichier de m√©tadonn√©es fourni. M√©tadonn√©es automatiques uniquement.")

    # Traiter les fichiers
    results = {"success": [], "errors": []}

    for idx, file_path in enumerate(file_paths, 1):
        logger.info(f"\n[{idx}/{len(file_paths)}] Traitement en cours...")

        # R√©cup√©rer les m√©tadonn√©es pour ce fichier
        file_metadata = metadata_map.get(file_path, {})

        result = process_with_metadata(
            file_path=file_path,
            metadata=file_metadata,
            embedding_gen=embedding_gen,
            uploader=uploader,
            ocr_processor=ocr_processor
        )

        if result["status"] == "success":
            results["success"].append(result)
        else:
            results["errors"].append(result)

    # R√©sum√©
    logger.info(f"\n{'='*70}")
    logger.info(f"üìä R√âSUM√â")
    logger.info(f"{'='*70}")
    logger.info(f"‚úÖ Succ√®s: {len(results['success'])}")
    logger.info(f"‚ùå Erreurs: {len(results['errors'])}")

    if results['success']:
        logger.info(f"\nüìä Exemples de m√©tadonn√©es utilis√©es:")
        for res in results['success'][:3]:
            logger.info(f"\n   {res['file_name']}:")
            logger.info(f"   - {res['metadata_fields']} champs de m√©tadonn√©es")
            logger.info(f"   - √âchantillon: {res['sample_metadata']}")

    if results['errors']:
        logger.info(f"\n‚ùå Fichiers en erreur:")
        for error in results['errors']:
            logger.info(f"   - {error['file_name']}: {error['error']}")


if __name__ == "__main__":
    main()
