#!/usr/bin/env python3
"""
Script de d√©monstration des niveaux de granularit√© de chunking.

Ce script permet de :
1. Visualiser comment diff√©rents niveaux de granularit√© affectent le d√©coupage du texte
2. Comparer le nombre de chunks g√©n√©r√©s pour chaque niveau
3. Estimer l'impact sur le co√ªt et la pr√©cision de recherche
"""

import sys
from pathlib import Path
from src.embeddings import EmbeddingGenerator
from src.chunking_config import (
    chunking_manager,
    GranularityLevel,
    GRANULARITY_CONFIGS
)


# Texte d'exemple pour la d√©monstration
SAMPLE_TEXT = """
L'intelligence artificielle (IA) est un domaine de l'informatique qui vise √† cr√©er des machines
capables de simuler l'intelligence humaine. Elle englobe plusieurs sous-domaines, notamment
l'apprentissage automatique (machine learning), le traitement du langage naturel, la vision
par ordinateur et la robotique.

Le machine learning est une branche de l'IA qui permet aux ordinateurs d'apprendre √† partir
de donn√©es sans √™tre explicitement programm√©s. Les algorithmes d'apprentissage automatique
peuvent identifier des mod√®les dans les donn√©es et faire des pr√©dictions ou des d√©cisions
bas√©es sur ces mod√®les.

Le deep learning, une sous-cat√©gorie du machine learning, utilise des r√©seaux de neurones
artificiels avec plusieurs couches pour traiter des donn√©es complexes. Ces r√©seaux sont
inspir√©s de la structure du cerveau humain et sont particuli√®rement efficaces pour des t√¢ches
comme la reconnaissance d'images, la traduction automatique et la g√©n√©ration de texte.

Les applications de l'IA sont vastes et touchent de nombreux domaines : sant√© (diagnostic
m√©dical, d√©couverte de m√©dicaments), finance (d√©tection de fraude, trading algorithmique),
transport (v√©hicules autonomes), commerce (recommandations personnalis√©es), et bien d'autres.

Cependant, l'IA soul√®ve √©galement des questions √©thiques importantes concernant la vie priv√©e,
la s√©curit√©, les biais algorithmiques et l'impact sur l'emploi. Il est crucial de d√©velopper
ces technologies de mani√®re responsable et transparente, en tenant compte de leurs implications
sociales et √©conomiques.

L'avenir de l'IA promet des avanc√©es r√©volutionnaires dans de nombreux domaines. Les chercheurs
travaillent sur des syst√®mes d'IA plus g√©n√©raux (AGI - Artificial General Intelligence) capables
de comprendre et d'apprendre n'importe quelle t√¢che intellectuelle qu'un humain peut accomplir.
Bien que nous soyons encore loin de cet objectif, les progr√®s r√©cents sont remarquables.

Les embeddings textuels, comme ceux utilis√©s dans ce syst√®me, sont un exemple parfait de
l'application pratique de l'IA moderne. Ils permettent de repr√©senter le sens s√©mantique
d'un texte sous forme de vecteurs num√©riques, facilitant ainsi la recherche et la comparaison
de documents bas√©es sur leur contenu plut√¥t que sur de simples correspondances de mots-cl√©s.
"""


def print_separator(char="=", length=80):
    """Affiche une ligne de s√©paration."""
    print(char * length)


def print_header(text):
    """Affiche un en-t√™te format√©."""
    print_separator()
    print(f"  {text}")
    print_separator()


def analyze_chunks(chunks, chunk_size, overlap, level_name):
    """Analyse et affiche les statistiques des chunks."""
    print(f"\nüìä {level_name}")
    print(f"   Param√®tres : chunk_size={chunk_size}, overlap={overlap}")
    print(f"   Nombre de chunks : {len(chunks)}")

    if chunks:
        avg_size = sum(len(c) for c in chunks) / len(chunks)
        min_size = min(len(c) for c in chunks)
        max_size = max(len(c) for c in chunks)

        print(f"   Taille moyenne des chunks : {avg_size:.0f} caract√®res")
        print(f"   Taille min/max : {min_size}/{max_size} caract√®res")


def estimate_costs(num_chunks):
    """Estime les co√ªts approximatifs pour les embeddings."""
    # Prix approximatifs pour text-embedding-3-small (janvier 2025)
    # $0.020 par 1M tokens (~750k mots)
    # Estimation : ~1.3 caract√®res = 1 token

    tokens_per_chunk = 300  # Estimation moyenne
    total_tokens = num_chunks * tokens_per_chunk
    cost_per_1m_tokens = 0.020

    estimated_cost_per_1k_docs = (total_tokens * cost_per_1m_tokens / 1_000_000) * 1000

    return {
        "total_tokens": total_tokens,
        "cost_per_doc": (total_tokens * cost_per_1m_tokens / 1_000_000),
        "cost_per_1k_docs": estimated_cost_per_1k_docs
    }


def compare_all_levels():
    """Compare tous les niveaux de granularit√© disponibles."""

    print_header("COMPARAISON DES NIVEAUX DE GRANULARIT√â")

    print(f"\nüìù Texte d'exemple : {len(SAMPLE_TEXT)} caract√®res")
    print(f"   (~{len(SAMPLE_TEXT.split())} mots)")

    # Initialiser le g√©n√©rateur d'embeddings
    # Note: Pas besoin de cl√© API r√©elle pour juste tester le chunking
    try:
        embedding_gen = EmbeddingGenerator(api_key="demo-key-not-used")
    except:
        # Si √ßa √©choue, cr√©er une instance simple sans API
        embedding_gen = EmbeddingGenerator.__new__(EmbeddingGenerator)

    print("\n")
    print_separator("-")
    print("ANALYSE PAR NIVEAU DE GRANULARIT√â")
    print_separator("-")

    results = []

    # Tester chaque niveau de granularit√©
    for level in GranularityLevel:
        config = GRANULARITY_CONFIGS[level]

        # D√©couper le texte
        chunks = embedding_gen.chunk_text(
            SAMPLE_TEXT,
            chunk_size=config.chunk_size,
            overlap=config.overlap
        )

        # Analyser
        analyze_chunks(
            chunks,
            config.chunk_size,
            config.overlap,
            level.value.upper()
        )

        # Estimer les co√ªts
        costs = estimate_costs(len(chunks))

        print(f"   üí∞ Co√ªt estim√© : ${costs['cost_per_doc']:.6f} par document")
        print(f"   üí∞ Co√ªt pour 1000 docs : ${costs['cost_per_1k_docs']:.2f}")
        print(f"   ‚ÑπÔ∏è  {config.description}")

        results.append({
            "level": level.value,
            "chunks": len(chunks),
            "config": config,
            "costs": costs
        })

    # Tableau r√©capitulatif
    print("\n")
    print_separator("=")
    print("TABLEAU R√âCAPITULATIF")
    print_separator("=")

    print(f"\n{'Niveau':<15} {'Chunks':<10} {'Taille':<12} {'Overlap':<12} {'Co√ªt/1k docs':<15}")
    print_separator("-")

    for result in results:
        level = result['level']
        chunks = result['chunks']
        config = result['config']
        cost = result['costs']['cost_per_1k_docs']

        print(
            f"{level.upper():<15} "
            f"{chunks:<10} "
            f"{config.chunk_size:<12} "
            f"{config.overlap:<12} "
            f"${cost:<14.2f}"
        )

    # Recommandations
    print("\n")
    print_separator("=")
    print("RECOMMANDATIONS")
    print_separator("=")

    print("""
üéØ ULTRA_FINE (200/50) :
   ‚úÖ Meilleure pr√©cision de recherche s√©mantique
   ‚úÖ Id√©al pour documents techniques d√©taill√©s
   ‚ö†Ô∏è  Co√ªt le plus √©lev√©
   ‚ö†Ô∏è  Plus de chunks = plus de temps de traitement

üéØ FINE (400/100) - RECOMMAND√â :
   ‚úÖ Excellent √©quilibre pr√©cision/co√ªt
   ‚úÖ Tr√®s bonne granularit√© pour la plupart des cas
   ‚úÖ Configuration V2 actuelle
   ‚úì  Bon rapport qualit√©/prix

üéØ MEDIUM (600/150) :
   ‚úÖ Bon compromis pour usage g√©n√©ral
   ‚úì  Co√ªt mod√©r√©
   ‚ö†Ô∏è  Moins de pr√©cision que FINE

üéØ STANDARD (1000/200) :
   ‚úÖ Bonne performance pour documents longs
   ‚úÖ Co√ªt r√©duit
   ‚ö†Ô∏è  Configuration V1 (ancienne)
   ‚ö†Ô∏è  Moins de granularit√©

üéØ COARSE (1500/300) :
   ‚úÖ Pour tr√®s gros corpus de documents
   ‚úÖ Co√ªt minimal
   ‚ö†Ô∏è  Perte significative de pr√©cision
   ‚ö†Ô∏è  Contexte tr√®s large peut diluer le sens

üí° CONSEIL : Pour maximiser la qualit√© de recherche s√©mantique, utilisez ULTRA_FINE ou FINE.
             Le surco√ªt est g√©n√©ralement n√©gligeable par rapport aux b√©n√©fices en pr√©cision.
""")

    print_separator("=")


def show_chunk_preview():
    """Affiche un aper√ßu des premiers chunks pour ULTRA_FINE vs STANDARD."""

    print_header("APER√áU VISUEL : ULTRA_FINE vs STANDARD")

    try:
        embedding_gen = EmbeddingGenerator(api_key="demo-key-not-used")
    except:
        embedding_gen = EmbeddingGenerator.__new__(EmbeddingGenerator)

    # ULTRA_FINE
    print("\nüîπ ULTRA_FINE (200 caract√®res, overlap 50)")
    print_separator("-", 80)

    ultra_config = GRANULARITY_CONFIGS[GranularityLevel.ULTRA_FINE]
    ultra_chunks = embedding_gen.chunk_text(
        SAMPLE_TEXT,
        chunk_size=ultra_config.chunk_size,
        overlap=ultra_config.overlap
    )

    for i, chunk in enumerate(ultra_chunks[:3]):  # Afficher 3 premiers
        print(f"\nChunk {i+1}/{len(ultra_chunks)} ({len(chunk)} chars):")
        print(f"  ¬´{chunk[:150]}{'...' if len(chunk) > 150 else ''}¬ª")

    print(f"\n... ({len(ultra_chunks) - 3} autres chunks)")

    # STANDARD
    print("\n")
    print("üîπ STANDARD (1000 caract√®res, overlap 200)")
    print_separator("-", 80)

    standard_config = GRANULARITY_CONFIGS[GranularityLevel.STANDARD]
    standard_chunks = embedding_gen.chunk_text(
        SAMPLE_TEXT,
        chunk_size=standard_config.chunk_size,
        overlap=standard_config.overlap
    )

    for i, chunk in enumerate(standard_chunks[:2]):  # Afficher 2 premiers
        print(f"\nChunk {i+1}/{len(standard_chunks)} ({len(chunk)} chars):")
        print(f"  ¬´{chunk[:200]}{'...' if len(chunk) > 200 else ''}¬ª")

    print(f"\n... ({len(standard_chunks) - 2} autres chunks)")

    print("\n")
    print("üí° OBSERVATION :")
    print(f"   - ULTRA_FINE : {len(ultra_chunks)} chunks tr√®s cibl√©s")
    print(f"   - STANDARD : {len(standard_chunks)} chunks plus g√©n√©raux")
    print(f"   - Ratio : {len(ultra_chunks)/len(standard_chunks):.1f}x plus de chunks avec ULTRA_FINE")
    print()


def main():
    """Point d'entr√©e principal."""

    print("\n" * 2)

    # Comparaison compl√®te
    compare_all_levels()

    # Aper√ßu visuel
    print("\n" * 2)
    show_chunk_preview()

    # Configuration actuelle
    print("\n" * 2)
    print_header("CONFIGURATION ACTUELLE DU SYST√àME")

    current_config = chunking_manager.get_config()
    current_level = chunking_manager.get_granularity_level()

    print(f"\n‚úÖ Niveau actif : {current_level.value.upper()}")
    print(f"   Chunk size : {current_config.chunk_size} caract√®res")
    print(f"   Overlap : {current_config.overlap} caract√®res")
    print(f"   Chunks/10k : ~{current_config.chunks_per_10k}")
    print(f"   Description : {current_config.description}")

    print(f"\nüí° Pour changer le niveau, modifiez GRANULARITY_LEVEL dans .env")
    print(f"   Niveaux disponibles : ULTRA_FINE, FINE, MEDIUM, STANDARD, COARSE")
    print()


if __name__ == "__main__":
    main()
